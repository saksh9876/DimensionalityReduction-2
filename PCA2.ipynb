{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b12aa0-dddf-4b27-a23c-4dd6c876931c",
   "metadata": {},
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5c0c7-1647-4bae-878e-291f64187065",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    A1. In the context of Principal Component Analysis (PCA), a projection refers to the transformation of data from a higher-dimensional space into a lower-dimensional space while retaining as much of the original data's variance as possible. PCA is a dimensionality reduction technique commonly used in data analysis and machine learning to simplify complex datasets by finding a set of orthogonal axes (principal components) along which the data varies the most.\n",
    "\n",
    "Here's how projections are used in PCA:\n",
    "\n",
    "1. Data Centering: The first step in PCA is to center the data by subtracting the mean of each feature from all data points. Centering ensures that the first principal component (PC) describes the direction of maximum variance.\n",
    "\n",
    "2. Covariance Matrix: PCA calculates the covariance matrix of the centered data. This matrix represents the relationships between different features and how they vary together. The diagonal elements of the covariance matrix represent the variances of individual features, while the off-diagonal elements represent the covariances between pairs of features.\n",
    "\n",
    "3. Eigendecomposition: PCA then performs eigendecomposition on the covariance matrix to obtain its eigenvalues and eigenvectors. The eigenvalues represent the amount of variance explained by each principal component, while the eigenvectors represent the direction (or axis) of each principal component.\n",
    "\n",
    "4. Selecting Principal Components: The eigenvalues are used to rank the principal components in descending order of importance. Typically, you retain the top k principal components that capture a significant portion of the total variance (e.g., 95% of the variance).\n",
    "\n",
    "5. Projection: The retained principal components form a new basis for the data. To reduce the dimensionality of the data, you project the original data points onto this new basis. Each data point is represented as a linear combination of the selected principal components. This projection effectively reduces the dimensionality of the data while preserving as much variance as possible.\n",
    "\n",
    "By projecting the data onto a lower-dimensional subspace defined by the selected principal components, PCA allows you to reduce the noise and redundancy in the data while retaining the essential information needed for analysis or modeling. It's a valuable tool for data preprocessing and feature extraction in various applications, such as image compression, pattern recognition, and dimensionality reduction in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456e4c9-703a-4f7e-b965-5bd1f3709dea",
   "metadata": {},
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fd5dd-c295-4672-a3c9-4d260c49735e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Principal Component Analysis (PCA) involves an optimization problem aimed at achieving dimensionality reduction while preserving as much variance as possible in the data. The central goal of this optimization problem is to find a set of orthogonal axes (principal components) onto which the data can be projected in a way that minimizes information loss. Here's how the optimization problem in PCA works and what it's trying to achieve:\n",
    "\n",
    "1. Objective Function: The optimization problem in PCA can be framed as maximizing the variance of the projected data. In mathematical terms, this is often expressed as maximizing the sum of the variances of the projected data points along the selected principal components.\n",
    "\n",
    "2. Principal Components: The optimization problem seeks to find a set of principal components (orthogonal unit vectors) that define a new basis for the data. These principal components are denoted as {PC1, PC2, PC3, ...} and are represented as eigenvectors of the covariance matrix of the centered data.\n",
    "\n",
    "3. Projection: The optimization problem involves selecting a subset of these principal components, typically in descending order of their corresponding eigenvalues. The first principal component (PC1) captures the direction of maximum variance in the data, the second principal component (PC2) captures the direction of the second most variance, and so on.\n",
    "\n",
    "4. Variance Maximization: The objective is to find a linear combination of the selected principal components to project the data onto a lower-dimensional subspace while maximizing the variance of the projected data. This linear combination is determined by the coefficients assigned to each principal component.\n",
    "\n",
    "Mathematically, for a given number k of principal components to retain (where k < the original dimensionality of the data), PCA tries to find a transformation matrix that maximizes the variance of the data when projected onto the k-dimensional subspace defined by the selected principal components.\n",
    "\n",
    "The optimization problem can be solved using techniques such as eigendecomposition or singular value decomposition (SVD) of the covariance matrix, where the eigenvectors corresponding to the top k eigenvalues are selected as the principal components. Alternatively, iterative methods like gradient descent can be used to optimize the projection matrix directly.\n",
    "\n",
    "In summary, the optimization problem in PCA aims to find a linear transformation of the data that reduces its dimensionality while preserving as much variance as possible. By selecting and retaining the top principal components, PCA achieves dimensionality reduction in a way that retains the most important information in the data and is widely used in various applications for feature extraction and data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2048a1f-7949-4fd7-b12c-cfb061d2d039",
   "metadata": {},
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412ea53-ed55-4cd6-988d-8b494818df0f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works. Covariance matrices play a crucial role in PCA, particularly in the calculation of principal components and the determination of the amount of variance captured by each component. Here's the relationship between covariance matrices and PCA:\n",
    "\n",
    "1. **Covariance Matrix:** In PCA, the first step is to compute the covariance matrix of the data. The covariance matrix, denoted as Σ (sigma), is a square matrix that describes the relationships between pairs of features in the dataset. Each element of the covariance matrix Σ(i, j) represents the covariance between the ith and jth features. The diagonal elements of the covariance matrix represent the variances of individual features.\n",
    "\n",
    "   Mathematically, for a dataset with n data points and m features, the covariance matrix Σ is calculated as:\n",
    "\n",
    "   \\[ \\Sigma = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(X_i - \\bar{X})^T \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(X_i\\) is the ith data point.\n",
    "   - \\(\\bar{X}\\) is the mean vector of the data.\n",
    "\n",
    "2. **Eigendecomposition:** After computing the covariance matrix, PCA proceeds to perform eigendecomposition on this matrix. Eigendecomposition is a mathematical technique that decomposes the covariance matrix into its eigenvectors and eigenvalues. The eigenvectors represent the directions (principal components) along which the data varies the most, while the eigenvalues correspond to the amount of variance explained by each principal component.\n",
    "\n",
    "3. **Principal Components:** The eigenvectors obtained from the eigendecomposition are the principal components of the data. These principal components are orthogonal to each other, and they form a new basis for the data. The first principal component corresponds to the direction of maximum variance in the data, the second principal component corresponds to the direction of the second most variance, and so on.\n",
    "\n",
    "4. **Variance Explained:** The eigenvalues associated with each principal component indicate the amount of variance explained by that component. PCA typically retains the top k principal components that capture the most variance, where k is a user-defined parameter. By summing the eigenvalues corresponding to the retained principal components and dividing by the total sum of eigenvalues, you can determine the proportion of variance retained in the reduced-dimensional representation.\n",
    "\n",
    "   \\[ \\text{Variance Explained by PC1 to PCk} = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{m} \\lambda_i} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(\\lambda_i\\) is the ith eigenvalue.\n",
    "\n",
    "In summary, covariance matrices are used in PCA to capture the relationships and variances between features in the data. The eigendecomposition of the covariance matrix yields the principal components, which allow for dimensionality reduction while preserving as much of the original data's variance as possible. The eigenvalues associated with these components provide insights into the amount of variance explained by each of them, aiding in the selection of the most informative components for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93837757-ffcd-4c24-9017-8ff130741e82",
   "metadata": {},
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33b0c1-9004-41b1-a04b-b18f33432572",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    The choice of the number of principal components (PCs) in Principal Component Analysis (PCA) has a significant impact on its performance and the results you obtain from dimensionality reduction. The number of PCs you select affects the trade-off between data compression (dimensionality reduction) and information retention. Here's how the choice of the number of principal components impacts PCA:\n",
    "\n",
    "1. **Dimensionality Reduction:** PCA is often used to reduce the dimensionality of data, especially in scenarios where you have a high-dimensional dataset with many features. By selecting a smaller number of principal components than the original dimensionality, you effectively reduce the complexity of the data representation.\n",
    "\n",
    "2. **Information Loss:** As you reduce the number of principal components, you retain less information from the original data. Each additional principal component explains a certain proportion of the total variance in the data. Therefore, if you select a smaller number of PCs, you will retain less of the original data's variance, which can lead to information loss.\n",
    "\n",
    "3. **Explained Variance:** One way to assess the impact of the number of PCs is by looking at the explained variance. Each principal component is associated with an eigenvalue, and the sum of eigenvalues represents the total variance in the data. The eigenvalues tell you how much variance each PC captures. By retaining a certain number of PCs, you can compute the proportion of variance in the data that is retained. A common guideline is to aim for a high percentage of explained variance (e.g., 95% or 99%) while reducing dimensionality.\n",
    "\n",
    "4. **Overfitting vs. Underfitting:** The choice of the number of principal components is similar to choosing the complexity of a model in machine learning. If you select too few PCs, you may underfit the data, meaning you lose important information, and your reduced-dimensional representation may not capture the data's structure effectively. On the other hand, if you select too many PCs, you may overfit the data, capturing noise and making your reduced representation less interpretable and less generalizable.\n",
    "\n",
    "5. **Computational Efficiency:** Using a smaller number of principal components reduces the computational cost of working with the data. This can be important when dealing with large datasets or when using PCA as a preprocessing step for other machine learning algorithms.\n",
    "\n",
    "In practice, the choice of the number of principal components often involves a trade-off between dimensionality reduction and information retention. It is common to plot the cumulative explained variance as a function of the number of PCs and select a threshold (e.g., 95% explained variance) to determine the optimal number of components. Cross-validation and other model evaluation techniques can also help in selecting an appropriate number of PCs based on the specific goals of your analysis or modeling task.\n",
    "\n",
    "In summary, the choice of the number of principal components in PCA impacts the balance between dimensionality reduction and information retention. It should be made based on a combination of domain knowledge, the desired level of explained variance, and considerations about computational efficiency and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a3979-edfe-42e9-9b9b-9921a22f55bf",
   "metadata": {},
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2740d-c827-4192-ac9a-74de1778ce2a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Principal Component Analysis (PCA) can be used in feature selection as a technique to identify and retain the most informative features while discarding less important or redundant ones. While PCA is primarily known for dimensionality reduction, it can indirectly serve as a feature selection method when used in a specific way. Here's how PCA can be employed for feature selection and its benefits:\n",
    "\n",
    "**Using PCA for Feature Selection:**\n",
    "\n",
    "1. **Standard PCA:** Initially, you apply standard PCA to your dataset with all its features to transform it into a new set of uncorrelated principal components. These components are linear combinations of the original features.\n",
    "\n",
    "2. **Variance Explained:** After performing PCA, you can examine the cumulative explained variance associated with each principal component. This information tells you how much of the total variance in the data is explained by each PC.\n",
    "\n",
    "3. **Feature Importance:** PCA indirectly ranks the importance of the original features based on how much of their variance is captured by the principal components. Features that contribute more to the variance will have higher importance in the PCA-based feature selection process.\n",
    "\n",
    "4. **Selecting Principal Components or Features:** You can then decide how many principal components (or equivalently, how many original features) to retain based on your desired level of explained variance. By retaining a certain number of components or features, you effectively select the most informative ones for your analysis or modeling.\n",
    "\n",
    "**Benefits of Using PCA for Feature Selection:**\n",
    "\n",
    "1. **Dimensionality Reduction:** PCA inherently reduces the dimensionality of the dataset by selecting a subset of principal components. This can be particularly beneficial when dealing with high-dimensional datasets, as it reduces computational complexity and mitigates the curse of dimensionality.\n",
    "\n",
    "2. **Noise Reduction:** PCA can help remove noise and redundancy in the data by focusing on the principal components that capture the most significant variations. This can lead to improved model generalization and interpretability.\n",
    "\n",
    "3. **Collinearity Handling:** When you have highly correlated features, PCA can capture the underlying patterns in a more meaningful way, reducing multicollinearity and making the feature space more interpretable.\n",
    "\n",
    "4. **Interpretability:** The selected principal components (or retained features) are linear combinations of the original features, making them potentially easier to interpret and analyze.\n",
    "\n",
    "5. **Improved Model Performance:** By selecting the most informative features using PCA, you can often improve the performance of machine learning models by reducing the dimensionality of the input space and focusing on the most relevant information.\n",
    "\n",
    "6. **Data Visualization:** PCA can also be used for data visualization. If you select a small number of principal components, you can create low-dimensional visualizations of the data to explore its structure and relationships.\n",
    "\n",
    "It's important to note that while PCA-based feature selection can be effective in many scenarios, it may not always be the best choice. In some cases, domain knowledge or specific feature selection techniques tailored to the problem may be more suitable. Additionally, the interpretability of the selected components or features should be considered, as PCA transforms the original features into linear combinations that may not have a direct physical or semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5b228-a581-49f1-9e31-20a3c52e5a4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9128f-9d2d-46ab-9942-eba75625321f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Principal Component Analysis (PCA) is a versatile technique widely used in data science and machine learning across various applications. Some common applications of PCA include:\n",
    "\n",
    "1. **Dimensionality Reduction:** PCA is primarily used for dimensionality reduction. It helps reduce the number of features (dimensions) in a dataset while preserving as much of the original data's variance as possible. This is valuable for simplifying complex datasets, speeding up algorithms, and mitigating the curse of dimensionality.\n",
    "\n",
    "2. **Feature Engineering:** PCA can be used for feature engineering to create new, uncorrelated features that capture the most significant variations in the data. These new features can then be used as input for machine learning models, potentially improving model performance.\n",
    "\n",
    "3. **Data Visualization:** PCA is employed for data visualization by reducing the data to two or three dimensions, making it easier to visualize and explore high-dimensional datasets. It is particularly useful for identifying clusters, patterns, and outliers in data.\n",
    "\n",
    "4. **Noise Reduction:** When data contains noise or redundancy, PCA can help reduce these unwanted variations by focusing on the principal components that capture meaningful information. This can improve the quality of data for downstream analysis.\n",
    "\n",
    "5. **Image Compression:** PCA is used in image compression techniques such as JPEG. By applying PCA to image data, you can reduce the number of pixels (dimensions) while preserving image quality, resulting in smaller file sizes.\n",
    "\n",
    "6. **Bioinformatics:** PCA is applied to analyze high-dimensional biological data, such as gene expression profiles. It helps identify patterns and relationships between genes, facilitating the study of gene expression in various conditions.\n",
    "\n",
    "7. **Finance:** In finance, PCA is used for risk management, portfolio optimization, and asset pricing. It can help identify the most influential factors or components affecting financial data.\n",
    "\n",
    "8. **Face Recognition:** PCA is used in face recognition systems to reduce the dimensionality of facial feature vectors while retaining essential facial characteristics. It aids in identifying and verifying individuals based on facial images.\n",
    "\n",
    "9. **Natural Language Processing (NLP):** In text and NLP applications, PCA can be applied to reduce the dimensionality of word embeddings or document-term matrices, making it easier to cluster, classify, or visualize text data.\n",
    "\n",
    "10. **Quality Control:** PCA is employed in quality control processes to monitor and improve the quality of manufacturing processes. It helps detect patterns and anomalies in sensor data from production lines.\n",
    "\n",
    "11. **Environmental Science:** PCA is used in environmental science to analyze complex environmental datasets, such as water quality measurements or climate data, to identify underlying trends and patterns.\n",
    "\n",
    "12. **Spectral Analysis:** In spectral data analysis, such as spectroscopy or remote sensing, PCA helps extract meaningful information from high-dimensional spectral datasets.\n",
    "\n",
    "13. **Anomaly Detection:** PCA can be used for anomaly detection by modeling the normal variation in data. Instances that deviate significantly from the norm may be flagged as anomalies.\n",
    "\n",
    "14. **Healthcare:** PCA is applied in healthcare for tasks like medical image analysis, patient data analysis, and disease diagnosis, where dimensionality reduction can be beneficial.\n",
    "\n",
    "These are just a few examples of how PCA is used in data science and machine learning. Its versatility and ability to uncover underlying structures and patterns in data make it a valuable tool in various domains. The choice to apply PCA depends on the specific problem and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667508ea-7ade-4a4e-ab63-f7832357a978",
   "metadata": {},
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b679a3-f980-47e3-8be1-c9db22c3e6b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    In the context of Principal Component Analysis (PCA), \"spread\" and \"variance\" are related concepts that describe how data points are distributed along the principal components. The spread of data points along a principal component is closely tied to the variance of those data points in the direction of that component. Here's how they are related:\n",
    "\n",
    "1. **Variance:** Variance is a statistical measure that quantifies the dispersion or spread of data points around their mean or center. In PCA, when we calculate the variance of data along a specific principal component (PC), we are essentially measuring how much the data varies along that direction. The larger the variance, the more spread out the data points are along the corresponding PC.\n",
    "\n",
    "2. **Principal Components:** In PCA, the principal components are orthogonal axes that represent directions of maximum variance in the data. The first principal component (PC1) corresponds to the direction of maximum variance, the second principal component (PC2) corresponds to the direction of the second most variance, and so on. PC1 captures the spread of data in the direction that explains the most variance.\n",
    "\n",
    "3. **Eigenvalues:** The eigenvalues associated with the principal components indicate the amount of variance explained by each component. The eigenvalue of a principal component reflects how \"spread out\" the data is along that direction. Higher eigenvalues correspond to greater variance explained by the corresponding PC.\n",
    "\n",
    "4. **Spread along Principal Components:** When you project your data onto the principal components (for dimensionality reduction or analysis), you are effectively spreading the data along these components. Data points that are far from the mean along a given PC direction contribute more to the variance in that direction, and data points that are close to the mean contribute less.\n",
    "\n",
    "5. **Variance Explained:** PCA allows you to rank the principal components by the amount of variance they capture. By selecting a subset of the top principal components, you can effectively reduce the dimensionality of your data while retaining the most significant sources of spread or variance in the dataset.\n",
    "\n",
    "In summary, in PCA, variance is a key concept that measures the spread or dispersion of data points along the principal components. Principal components are the directions of maximum variance, and the eigenvalues associated with these components quantify the amount of variance (or spread) explained by each direction. By selecting and retaining certain principal components, you control how much of the spread or variance in the data you preserve in the reduced-dimensional representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d090c1a-efe7-4212-ad88-fd6d63b737e5",
   "metadata": {},
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c67913-3008-4e33-98b3-dd5ed8d9aa5b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Principal Component Analysis (PCA) uses the spread and variance of the data to identify principal components by finding the directions along which the data varies the most. Here's how PCA utilizes spread and variance in the identification of principal components:\n",
    "\n",
    "1. **Covariance Matrix:** The first step in PCA is to compute the covariance matrix (\\( \\Sigma \\)) of the dataset. The covariance matrix summarizes the relationships between pairs of features and how they vary together. Specifically, the off-diagonal elements of the covariance matrix represent the covariances between pairs of features, and the diagonal elements represent the variances of individual features.\n",
    "\n",
    "2. **Eigenvalue Decomposition:** After obtaining the covariance matrix, PCA performs eigenvalue decomposition on this matrix. The eigenvalue decomposition breaks down the covariance matrix into its eigenvalues and eigenvectors.\n",
    "\n",
    "3. **Eigenvalues:** The eigenvalues (\\( \\lambda \\)) represent the amount of variance explained by each corresponding eigenvector (principal component). These eigenvalues are sorted in descending order. The eigenvalue associated with each principal component tells you how much of the total variance in the data is captured by that component. Larger eigenvalues indicate directions of maximum spread or variance.\n",
    "\n",
    "4. **Eigenvectors (Principal Components):** The eigenvectors represent the directions in which the data exhibits the most variation. These eigenvectors are normalized to be unit vectors and are sorted in the same order as their associated eigenvalues. The first eigenvector (PC1) corresponds to the direction of maximum variance, the second eigenvector (PC2) corresponds to the direction of the second most variance, and so on.\n",
    "\n",
    "5. **Selection of Principal Components:** You can choose to retain a subset of the top principal components based on the eigenvalues. Typically, you retain the principal components that capture a significant proportion of the total variance (e.g., retaining components until you reach a desired cumulative variance threshold, like 95% or 99% of the total variance).\n",
    "\n",
    "By considering the eigenvalues and eigenvectors, PCA identifies the principal components that represent the directions of maximum variance, which correspond to the directions along which the data spreads out the most. These principal components are used to construct a new basis for the data, providing a reduced-dimensional representation while retaining as much of the original data's variance as desired.\n",
    "\n",
    "In summary, PCA leverages the spread and variance of the data, as quantified by the covariance matrix and the eigenvalues of that matrix, to determine the principal components. These principal components are the directions in which the data exhibits the greatest variation, making them a valuable tool for dimensionality reduction and feature extraction in data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b552b-13c5-451b-9be6-9aae9ad7e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
